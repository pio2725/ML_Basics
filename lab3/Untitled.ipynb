{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "known-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses = arff.loadarff(\"lenses.arff\")\n",
    "lenses_df = pd.DataFrame(lenses[0])\n",
    "\n",
    "str_df = lenses_df.select_dtypes([np.object])\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "for col in str_df:\n",
    "    lenses_df[col]=str_df[col]\n",
    "dataset=lenses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "diverse-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def get_entropy(self,col):\n",
    "        n, counts = np.unique(col, return_counts=True)\n",
    "#         info_s = 0\n",
    "#         for i in range(len(n)):\n",
    "#             info_s += (-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts))\n",
    "        entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(n))])\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def get_gain(self,data, col_name, target_name):\n",
    "        info_s = self.get_entropy(data[target_name])\n",
    "        n, counts= np.unique(data[col_name],return_counts=True)\n",
    "\n",
    "        info_gain = np.sum([(counts[i]/np.sum(counts))*self.get_entropy(data.where(data[col_name]==n[i]).dropna()[target_name]) for i in range(len(n))])\n",
    "\n",
    "\n",
    "        #Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "\n",
    "\n",
    "        return info_s - info_gain\n",
    "\n",
    "    def ID3(self,data,originaldata,features,target_attribute_name=\"type\",parent_node_class = None):\n",
    "\n",
    "        if len(features) == 0:\n",
    "            return parent_node_class\n",
    "        n = np.unique(data[target_attribute_name])\n",
    "        if len(n) <= 1:\n",
    "            \n",
    "            return n[0]\n",
    "\n",
    "        #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "        if len(data)==0:\n",
    "            return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "        else:\n",
    "\n",
    "            target_counts = np.unique(data[target_attribute_name], return_counts=True)[1]\n",
    "            max_target_count = np.argmax(target_counts)\n",
    "            parent_node_class = np.unique(data[target_attribute_name])[max_target_count]\n",
    "\n",
    "\n",
    "            #Select the feature which best splits the dataset\n",
    "    #         info_gains = []\n",
    "    #         for i in features:\n",
    "    #             print(\"i = \", i, \" \", target_attribute_name)\n",
    "    #             info_gains.append(get_gain(data, i, target_attribute_name))\n",
    "    #         info_gains = np.array(info_gains)\n",
    "    #         best_arg = np.argmax(info_gains)\n",
    "    #         best_feature = features[best_arg]\n",
    "\n",
    "\n",
    "\n",
    "            item_values = [self.get_gain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "            best_feature_index = np.argmax(item_values)\n",
    "            best_feature = features[best_feature_index]\n",
    "            print(item_values[best_feature_index])\n",
    "            #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "            #gain in the first run\n",
    "            tree = {best_feature:{}}\n",
    "\n",
    "\n",
    "            #Remove the feature with the best inforamtion gain from the feature space\n",
    "            features = [i for i in features if i != best_feature]\n",
    "            #Grow a branch under the root node for each possible value of the root node feature\n",
    "\n",
    "            for value in np.unique(data[best_feature]):\n",
    "                value = value\n",
    "                #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "                sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "                #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "                subtree = self.ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "\n",
    "                #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "                tree[best_feature][value] = subtree\n",
    "\n",
    "            return tree    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,query,tree,default = 1):\n",
    "        #1.\n",
    "        for key in list(query.keys()):\n",
    "            if key in list(tree.keys()):\n",
    "                #2.\n",
    "                try:\n",
    "                    result = tree[key][query[key]] \n",
    "                except:\n",
    "                    return default\n",
    "\n",
    "                #3.\n",
    "                result = tree[key][query[key]]\n",
    "                #4.\n",
    "                if isinstance(result,dict):\n",
    "                    return self.predict(query,result)\n",
    "\n",
    "                else:\n",
    "                    return result\n",
    "                \n",
    "                \n",
    "                \n",
    "    def test(self,data,tree):\n",
    "        #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "        #convert it to a dictionary\n",
    "        queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "        \n",
    "        print(queries[0])\n",
    "\n",
    "        #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "        predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "\n",
    "        #Calculate the prediction accuracy\n",
    "        for i in range(len(data)):\n",
    "            predicted.loc[i,\"predicted\"] = self.predict(queries[i],tree,1.0) \n",
    "        print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"type\"])/len(data))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "every-testament",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-48bc32d602d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenses_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenses_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenses_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlenses_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlenses_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-88-a5c807199ee4>\u001b[0m in \u001b[0;36mID3\u001b[1;34m(self, data, originaldata, features, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparent_node_class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_attribute_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "# Load debug data\n",
    "lenses = arff.loadarff(\"lenses.arff\")\n",
    "lenses_df = pd.DataFrame(lenses[0])\n",
    "\n",
    "str_df = lenses_df.select_dtypes([np.object])\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "for col in str_df:\n",
    "    lenses_df[col]=str_df[col]\n",
    "    \n",
    "lenses_test = arff.loadarff(\"lenses_test.arff\")\n",
    "lenses_test_df = pd.DataFrame(lenses_test[0])\n",
    "str_df = lenses_test_df.select_dtypes([np.object])\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "for col in str_df:\n",
    "    lenses_test_df[col]=str_df[col]    \n",
    "lenses_np = np.array(lenses_df)\n",
    "X = lenses_np[:,:-1]\n",
    "y = lenses_np[:,-1]\n",
    "f = list(lenses_df.keys())[:4]\n",
    "dt = DT(lenses_df)\n",
    "tree = dt.ID3(lenses_df, lenses_df, lenses_df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "critical-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 'young', 'spectacle_prescrip': 'myope', 'astigmatism': 'no', 'tear_prod_rate': 'reduced'}\n",
      "The prediction accuracy is:  33.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "dt.test(lenses_test_df, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "apparent-polymer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normal': {'astigmatism': {'no': {'age': {'pre_presbyopic': 'soft',\n",
       "     'presbyopic': {'spectacle_prescrip': {'hypermetrope': 'none',\n",
       "       'myope': 'none'}},\n",
       "     'young': 'soft'}},\n",
       "   'yes': {'spectacle_prescrip': {'hypermetrope': {'age': {'pre_presbyopic': 'none',\n",
       "       'presbyopic': 'none',\n",
       "       'young': 'none'}},\n",
       "     'myope': 'hard'}}}},\n",
       " 'reduced': 'none'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree['tear_prod_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "forced-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree['tear_prod_rate']['reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "present-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3630469031539394\n",
      "0.8865408928220899\n",
      "0.9852281360342515\n",
      "0.6962122601251458\n",
      "0.8256265261578954\n",
      "0.6892019851173654\n",
      "0.8631205685666308\n",
      "0.7219280948873623\n",
      "0.7219280948873623\n",
      "{'hair': 'F', 'feathers': 'F', 'eggs': 'T', 'milk': 'T', 'airborne': 'T', 'predator': 'F', 'aquatic': 'F', 'toothed': 'F', 'backbone': 'T', 'breathes': 'T', 'venomous': 'F', 'fins': 'T', 'legs': '8', 'tails': 'F', 'domestic': 'F', 'catsize': 'T'}\n",
      "The prediction accuracy is:  14.7 %\n"
     ]
    }
   ],
   "source": [
    "zoo = arff.loadarff(\"zoo.arff\")\n",
    "zoo_df = pd.DataFrame(zoo[0])\n",
    "\n",
    "str_df = zoo_df.select_dtypes([np.object])\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "for col in str_df:\n",
    "    zoo_df[col]=str_df[col]\n",
    "zoo_np = np.array(zoo_df)\n",
    "X = zoo_np[:,-1]\n",
    "y = zoo_np[:-1]\n",
    "\n",
    "# Train Decision Tree\n",
    "\n",
    "# Load evaluation test data\n",
    "zoo_test = arff.loadarff(\"zoo_test.arff\")\n",
    "zoo_test_df = pd.DataFrame(zoo_test[0])\n",
    "\n",
    "str_df = zoo_test_df.select_dtypes([np.object])\n",
    "str_df = str_df.stack().str.decode('utf-8').unstack()\n",
    "for col in str_df:\n",
    "    zoo_test_df[col]=str_df[col]\n",
    "zoo_test_np = np.array(zoo_test_df)\n",
    "X_test = zoo_test_np[:,:-1]\n",
    "y_test = zoo_test_np[:,-1]\n",
    "\n",
    "dt = DT(zoo_df)\n",
    "tree = dt.ID3(zoo_df, zoo_df, zoo_df.columns[:-1])\n",
    "dt.test(zoo_test_df, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "burning-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3260875253642983\n"
     ]
    }
   ],
   "source": [
    "labels = list(lenses_df.keys())\n",
    "print((-15/24)*np.log2(15/24)+(-5/24)*np.log2(5/24)+(-4/24)*np.log2(4/24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "geological-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = CustomDecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abstract-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses_df.replace('young',0,inplace=True)\n",
    "lenses_df.replace('pre_presbyopic',1,inplace=True)\n",
    "lenses_df.replace('presbyopic',2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "exotic-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses_df.replace('myope',0,inplace=True)\n",
    "lenses_df.replace('hypermetrope',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "outstanding-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses_df.replace('no',0,inplace=True)\n",
    "lenses_df.replace('yes',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hollow-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses_df.replace('reduced',0,inplace=True)\n",
    "lenses_df.replace('normal',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "turned-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses_df.replace('none',0,inplace=True)\n",
    "lenses_df.replace('soft',1,inplace=True)\n",
    "lenses_df.replace('hard',2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stuffed-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3679197916065703 -1\n",
      "-0.21541354172676297 -0.3679197916065703\n",
      "-0.21541354172676297 -0.21541354172676297\n",
      "the best feature to split is spectacle_prescrip -0.21541354172676297\n",
      "-0.4025062498798073 -1\n",
      "-0.2641604167868593 -0.4025062498798073\n",
      "the best feature to split is astigmatism -0.2641604167868593\n",
      "-0.138345833092948 -1\n",
      "the best feature to split is age -0.138345833092948\n",
      "the best feature to split is tear_prod_rate -1\n",
      "the best feature to split is tear_prod_rate -1\n",
      "-0.138345833092948 -1\n",
      "the best feature to split is age -0.138345833092948\n",
      "the best feature to split is tear_prod_rate -1\n",
      "the best feature to split is tear_prod_rate -1\n",
      "the best feature to split is tear_prod_rate -1\n",
      "-0.4025062498798073 -1\n",
      "-0.2641604167868593 -0.4025062498798073\n",
      "the best feature to split is astigmatism -0.2641604167868593\n",
      "-0.138345833092948 -1\n",
      "the best feature to split is age -0.138345833092948\n",
      "the best feature to split is tear_prod_rate -1\n",
      "the best feature to split is tear_prod_rate -1\n",
      "the best feature to split is tear_prod_rate -1\n",
      "-0.138345833092948 -1\n",
      "the best feature to split is age -0.138345833092948\n",
      "the best feature to split is tear_prod_rate -1\n",
      "{'spectacle_prescrip': {0.0: {'astigmatism': {0.0: {'age': {0.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 1.0}}, 1.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 1.0}}, 2.0: 0.0}}, 1.0: {'age': {0.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 2.0}}, 1.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 2.0}}, 2.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 2.0}}}}}}, 1.0: {'astigmatism': {0.0: {'age': {0.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 1.0}}, 1.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 1.0}}, 2.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 1.0}}}}, 1.0: {'age': {0.0: {'tear_prod_rate': {0.0: 0.0, 1.0: 2.0}}, 1.0: 0.0, 2.0: 0.0}}}}}}\n"
     ]
    }
   ],
   "source": [
    "d = lenses_df.astype(float).values.tolist()\n",
    "print(dt.createTree(d, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ethical-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-galaxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit([1,2,2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tested-proposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "north-ancient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform([1,2,2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-sector",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
